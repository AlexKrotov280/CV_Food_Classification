{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "#from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.image as img\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "config.gpu_options.allow_growth = True\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "local_zip = 'train.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('train')\n",
    "local_zip = 'test.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('test')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24000 images belonging to 32 classes.\n",
      "Found 8000 images belonging to 32 classes.\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "sess = tf.Session(config=config)\n",
    "for d in ['/device:GPU:2', '/device:GPU:3']:\n",
    "    with tf.device(d):\n",
    "        nb_train_samples = 32*750\n",
    "        nb_validation_samples = 32*250\n",
    "        batch_size = 64\n",
    "        # All images will be rescaled by 1./255\n",
    "        train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "            shear_range=30, # 6-th from 25 to 30\n",
    "            zoom_range=0.5, # 7-th from 0.3 to 0.5\n",
    "            fill_mode='constant',\n",
    "            rotation_range=90, #6-rd itteration from 90 to 180\n",
    "            width_shift_range=.2, # 5-th from 0.5 to 0.25\n",
    "            height_shift_range=.2, # 5-th from 0.5 to 0.25\n",
    "            horizontal_flip=True)\n",
    "            #vertical_flip=True,\n",
    "            #channel_shift_range=100., 3-rd itteration\n",
    "            #brightness_range=[0.5,1.0], 6-th \n",
    "            #zca_whitening=True) # 6-th\n",
    "\n",
    "        validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "        # Flow training images in batches of 128 using train_datagen generator\n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "                'train/train/',  # This is the source directory for training images\n",
    "                target_size=(224, 224),  # All images will be resized to 150x150\n",
    "                batch_size = batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='categorical')\n",
    "\n",
    "        # Flow training images in batches of 128 using train_datagen generator\n",
    "        validation_generator = validation_datagen.flow_from_directory(\n",
    "                'test/test/',  # This is the source directory for training images\n",
    "                target_size=(224, 224),  # All images will be resized to 150x150\n",
    "                batch_size = batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='categorical')\n",
    "        inception = MobileNetV2(weights='imagenet', include_top=False, input_shape = (224,224,3))\n",
    "        #for layer in inception.layers:\n",
    "        #  layer.trainable = False\n",
    "        x = inception.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(128,activation='relu')(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "\n",
    "        predictions = Dense(32,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n",
    "        model = Model(inputs=inception.input, outputs=predictions)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=SGD(lr=0.001, momentum=0.9, decay=0.001), metrics=['accuracy'])\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='MobileNetV2_32classes.hdf5', verbose=1, monitor='val_loss', save_best_only=True)\n",
    "csv_logger = CSVLogger('history_MobileNetv2_32classes.log')\n",
    "es = EarlyStopping(monitor='val_acc', mode='max', patience=10, verbose=1, baseline=None)\n",
    "tb = TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=64, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "374/375 [============================>.] - ETA: 1s - loss: 2.8753 - acc: 0.2885\n",
      "Epoch 00001: val_loss improved from inf to 2.16963, saving model to MobileNetV2_32classes.hdf5\n",
      "375/375 [==============================] - 548s 1s/step - loss: 2.8747 - acc: 0.2886 - val_loss: 2.1696 - val_acc: 0.4441\n",
      "Epoch 2/80\n",
      "374/375 [============================>.] - ETA: 0s - loss: 1.9678 - acc: 0.5056\n",
      "Epoch 00002: val_loss improved from 2.16963 to 1.68388, saving model to MobileNetV2_32classes.hdf5\n",
      "375/375 [==============================] - 456s 1s/step - loss: 1.9679 - acc: 0.5056 - val_loss: 1.6839 - val_acc: 0.5833\n",
      "Epoch 3/80\n",
      "374/375 [============================>.] - ETA: 1s - loss: 1.7163 - acc: 0.5697\n",
      "Epoch 00003: val_loss improved from 1.68388 to 1.45984, saving model to MobileNetV2_32classes.hdf5\n",
      "375/375 [==============================] - 508s 1s/step - loss: 1.7160 - acc: 0.5700 - val_loss: 1.4598 - val_acc: 0.6420\n",
      "Epoch 4/80\n",
      "374/375 [============================>.] - ETA: 1s - loss: 1.5812 - acc: 0.6065\n",
      "Epoch 00004: val_loss improved from 1.45984 to 1.25752, saving model to MobileNetV2_32classes.hdf5\n",
      "375/375 [==============================] - 555s 1s/step - loss: 1.5811 - acc: 0.6066 - val_loss: 1.2575 - val_acc: 0.6914\n",
      "Epoch 5/80\n",
      "374/375 [============================>.] - ETA: 1s - loss: 1.5091 - acc: 0.6259\n",
      "Epoch 00005: val_loss improved from 1.25752 to 1.13365, saving model to MobileNetV2_32classes.hdf5\n",
      "375/375 [==============================] - 515s 1s/step - loss: 1.5087 - acc: 0.6261 - val_loss: 1.1336 - val_acc: 0.7224\n",
      "Epoch 6/80\n",
      "374/375 [============================>.] - ETA: 1s - loss: 1.4282 - acc: 0.6454\n",
      "Epoch 00006: val_loss improved from 1.13365 to 1.04817, saving model to MobileNetV2_32classes.hdf5\n",
      "375/375 [==============================] - 612s 2s/step - loss: 1.4283 - acc: 0.6454 - val_loss: 1.0482 - val_acc: 0.7408\n",
      "Epoch 7/80\n",
      "365/375 [============================>.] - ETA: 12s - loss: 1.3874 - acc: 0.6552"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch = nb_train_samples // batch_size,\n",
    "      validation_data = validation_generator,\n",
    "      validation_steps = nb_validation_samples // batch_size,  \n",
    "      epochs=80,\n",
    "      verbose=1,\n",
    "      callbacks=[csv_logger, checkpointer, es, tb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_accuracy(history,title):\n",
    "    plt.title(title)\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train_accuracy', 'validation_accuracy'], loc='best')\n",
    "    plt.show()\n",
    "def plot_loss(history,title):\n",
    "    plt.title(title)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train_loss', 'validation_loss'], loc='best')\n",
    "    plt.show()\n",
    "plot_accuracy(history,'FOOD101_32 classes')\n",
    "plot_loss(history,'FOOD101_32 classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
